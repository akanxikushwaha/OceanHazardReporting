{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install supabase\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRrho8iipShS",
        "outputId": "e9c0be72-343e-445a-e8cd-abbc81bc4b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: supabase in /usr/local/lib/python3.12/dist-packages (2.20.0)\n",
            "Requirement already satisfied: realtime in /usr/local/lib/python3.12/dist-packages (from supabase) (2.20.0)\n",
            "Requirement already satisfied: supabase-functions in /usr/local/lib/python3.12/dist-packages (from supabase) (2.20.0)\n",
            "Requirement already satisfied: storage3 in /usr/local/lib/python3.12/dist-packages (from supabase) (2.20.0)\n",
            "Requirement already satisfied: supabase-auth in /usr/local/lib/python3.12/dist-packages (from supabase) (2.20.0)\n",
            "Requirement already satisfied: postgrest in /usr/local/lib/python3.12/dist-packages (from supabase) (2.20.0)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.12/dist-packages (from supabase) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.16.0)\n",
            "Requirement already satisfied: deprecation>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (2.1.0)\n",
            "Requirement already satisfied: pydantic<3.0,>=1.9 in /usr/local/lib/python3.12/dist-packages (from postgrest->supabase) (2.11.9)\n",
            "Requirement already satisfied: typing-extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from realtime->supabase) (4.15.0)\n",
            "Requirement already satisfied: websockets<16,>=11 in /usr/local/lib/python3.12/dist-packages (from realtime->supabase) (15.0.1)\n",
            "Requirement already satisfied: pyjwt>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.10.1)\n",
            "Requirement already satisfied: strenum>=0.4.15 in /usr/local/lib/python3.12/dist-packages (from supabase-functions->supabase) (0.4.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from deprecation>=2.1.0->postgrest->supabase) (25.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[http2]<0.29,>=0.26->postgrest->supabase) (4.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0,>=1.9->postgrest->supabase) (0.4.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (43.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.0.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->postgrest->supabase) (4.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->supabase-auth->supabase) (2.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFNmFnVrU5BJ",
        "outputId": "706fdfca-873b-419a-cf2a-bf702db3f707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import html\n",
        "import logging\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "from textblob import TextBlob\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "    from langdetect import detect\n",
        "except:\n",
        "    def detect(x):\n",
        "        return \"unknown\"\n",
        "\n",
        "from supabase import create_client, Client\n",
        "\n",
        "# -------------------------\n",
        "# For location extraction\n",
        "# -------------------------\n",
        "import spacy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")  # English NER\n",
        "except:\n",
        "    import spacy.cli\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# -------------------------\n",
        "# Supabase Config\n",
        "# -------------------------\n",
        "SUPABASE_URL = \"https://fqjoohzcvsvlfmjcucmq.supabase.co\"\n",
        "SUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZxam9vaHpjdnN2bGZtamN1Y21xIiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1ODMxMTM4MiwiZXhwIjoyMDczODg3MzgyfQ.no3TuB042a_PHT3MY1PBucV2s7SuGsfzaIK8Kl7jlIc\"\n",
        "TABLE_NAME = \"twitterdata\"\n",
        "\n",
        "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "# -------------------------\n",
        "# Logging\n",
        "# -------------------------\n",
        "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
        "                    level=logging.ERROR,\n",
        "                    handlers=[logging.StreamHandler()])\n",
        "\n",
        "# -------------------------\n",
        "# Load Models\n",
        "# -------------------------\n",
        "embed_model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"joeddav/xlm-roberta-large-xnli\")\n",
        "literal_clf = joblib.load(\"/content/drive/MyDrive/Colab Notebooks/SIH/literal_figurative_clf (1).pkl\")\n",
        "\n",
        "# -------------------------\n",
        "# Hazard keywords / concepts\n",
        "# -------------------------\n",
        "hazard_concepts = [\n",
        "    # English\n",
        "    \"There is a tsunami warning\",\n",
        "    \"Storm surge expected on the coast\",\n",
        "    \"Dangerous rip currents reported\",\n",
        "    \"High tide flooding near beaches\",\n",
        "    \"Waves are extremely rough\",\n",
        "\n",
        "    # Spanish\n",
        "    \"Alerta de tsunami en la costa\",\n",
        "    \"Marejada ciclónica en camino\",\n",
        "    \"Corrientes peligrosas en la playa\",\n",
        "\n",
        "    # Tagalog\n",
        "    \"May babala ng tsunami\",\n",
        "    \"Malakas ang alon sa tabing-dagat\",\n",
        "    \"Mapanganib ang mga alon ngayon\",\n",
        "\n",
        "    # Hindi\n",
        "    \"सुनामी की चेतावनी जारी की गई है\",\n",
        "    \"तट पर तेज लहरें आ रही हैं\",\n",
        "    \"समुद्र बहुत उग्र है\",\n",
        "\n",
        "    # French\n",
        "    \"Alerte de vague géante\",\n",
        "    \"Inondation côtière imminente\",\n",
        "    \"Mer très agitée\",\n",
        "\n",
        "    # Japanese\n",
        "    \"津波警報が発令されました\",\n",
        "    \"海がとても荒れています\",\n",
        "\n",
        "    # Portuguese\n",
        "    \"Alerta de correnteza no mar\",\n",
        "    \"Ondas perigosas estão chegando\"\n",
        "]\n",
        "hazard_embeddings = embed_model.encode(hazard_concepts, convert_to_tensor=True)\n",
        "\n",
        "ocean_hazard_keywords = [\n",
        "    # General\n",
        "    \"ocean hazard\", \"marine hazard\", \"coastal hazard\", \"sea hazard\",\n",
        "    \"abnormal sea\", \"abnormal ocean\", \"abnormal tide\", \"dangerous sea\", \"rough sea\",\n",
        "    # Tsunami\n",
        "    \"tsunami\", \"tidal wave\", \"seismic sea wave\", \"submarine earthquake\",\n",
        "    \"tsunami alert\", \"tsunami warning\", \"tsunami surge\", \"tsunami waves\",\n",
        "    \"undersea quake\", \"tsunami advisory\", \"wave run-up\", \"tsunami evacuation\",\n",
        "    # Storm surge\n",
        "    \"storm surge\", \"hurricane surge\", \"cyclone surge\", \"typhoon surge\", \"surge flood\",\n",
        "    \"wind-driven surge\", \"surge tide\", \"storm tide\", \"surge inundation\",\n",
        "    # Coastal flooding\n",
        "    \"coastal flooding\", \"coastal inundation\", \"coastal submersion\", \"flooded coast\",\n",
        "    \"high tide flood\", \"marine flooding\", \"ocean flooding\", \"seawater intrusion\",\n",
        "    # Waves & swells\n",
        "    \"high waves\", \"rogue wave\", \"monster wave\", \"ocean swell\", \"sea swell\",\n",
        "    \"swelling sea\", \"dangerous waves\", \"huge waves\", \"wave inundation\",\n",
        "    \"long-period swell\", \"storm waves\", \"wave runup\", \"breaking waves\", \"swell surge\",\n",
        "    # Currents\n",
        "    \"coastal current\", \"rip current\", \"rip tide\", \"undertow\", \"longshore current\",\n",
        "    \"dangerous current\", \"fast-moving current\", \"marine current\", \"backwash\",\n",
        "    \"cross current\", \"nearshore current\", \"beach current\", \"offshore pull\",\n",
        "    # Tides\n",
        "    \"king tide\", \"strong tide\", \"spring tide\", \"tidal bore\", \"rising tide\",\n",
        "    \"falling tide\", \"tidal anomaly\", \"extreme tide\", \"abnormal tide\",\n",
        "    # Erosion & marine impact\n",
        "    \"coastal erosion\", \"shoreline erosion\", \"beach erosion\", \"dune erosion\",\n",
        "    \"seawall overtopping\", \"pier damage\", \"breakwater breach\",\n",
        "    # Sea level rise\n",
        "    \"sea level rise\", \"rising sea\", \"climate-induced flooding\", \"marine transgression\",\n",
        "    # Other marine dangers\n",
        "    \"floating debris\", \"marine debris\", \"navigation hazard\", \"maritime hazard\",\n",
        "    \"submarine landslide\", \"seafloor uplift\", \"coastal collapse\", \"land subsidence\",\n",
        "    \"wave reflection\", \"wave resonance\", \"harbor resonance\", \"harbor oscillation\",\n",
        "    # Warnings & alerts\n",
        "    \"marine warning\", \"ocean alert\", \"coastal alert\", \"beach hazard statement\",\n",
        "    \"rip current warning\", \"tsunami watch\", \"storm surge watch\", \"high surf advisory\",\n",
        "    \"coastal flood advisory\", \"surf warning\", \"maritime safety warning\",\n",
        "    # Human impact phrases\n",
        "    \"beach closed\", \"swimming prohibited\", \"evacuation order\", \"life-threatening waves\",\n",
        "    \"drowning risk\", \"hazardous beach conditions\", \"boats damaged\", \"ports closed\",\n",
        "    # Informal/common\n",
        "    \"crazy waves\", \"killer wave\", \"ocean acting weird\", \"sea looks angry\", \"strange tide\",\n",
        "    \"waves pulling people\", \"water rushing in\", \"freak wave\", \"unusual sea\", \"weird ocean activity\"\n",
        "]\n",
        "\n",
        "# -------------------------\n",
        "# Severity templates\n",
        "# -------------------------\n",
        "severity_templates = {\n",
        "    \"High severity ocean hazard\": [\n",
        "        \"Tsunami caused widespread coastal destruction\",\n",
        "        \"Storm surge flooded entire coastal towns\",\n",
        "        \"Abnormally high waves damaged infrastructure and caused fatalities\"\n",
        "    ],\n",
        "    \"Medium severity ocean hazard\": [\n",
        "        \"Storm surge expected to cause moderate flooding\",\n",
        "        \"Tsunami warning issued after offshore earthquake\",\n",
        "        \"Strong rip currents along beaches with injury reports\"\n",
        "    ],\n",
        "    \"Low severity ocean hazard\": [\n",
        "        \"High tide and coastal current advisory issued\",\n",
        "        \"Waves expected to be higher than usual, swimming not advised\",\n",
        "        \"Minor coastal flooding expected due to sea level anomaly\"\n",
        "    ]\n",
        "}\n",
        "template_embeddings = {sev: embed_model.encode(sents, convert_to_tensor=True)\n",
        "                       for sev, sents in severity_templates.items()}\n",
        "\n",
        "# -------------------------\n",
        "# Cleaner\n",
        "# -------------------------\n",
        "URL_PATTERN = re.compile(r\"https?://\\S+\")\n",
        "NEGATION_WORDS = [\"no\", \"not\", \"without\", \"sin\", \"kein\", \"无\", \"不要\", \"pas\", \"aucun\"]\n",
        "hazard_pattern = r\"|\".join([re.escape(k) for k in ocean_hazard_keywords])\n",
        "negation_pattern = r\"\\b(\" + r\"|\".join(NEGATION_WORDS) + r\")\\s+(\" + hazard_pattern + r\")\\b\"\n",
        "NEGATION_PATTERNS = re.compile(negation_pattern, flags=re.IGNORECASE)\n",
        "\n",
        "def clean_text_better(text):\n",
        "    text = html.unescape(text)\n",
        "    text = re.sub(r\"^RT\\s+@[\\w_]+:\\s*\", \"\", text, flags=re.IGNORECASE)\n",
        "    urls = URL_PATTERN.findall(text)\n",
        "    text = URL_PATTERN.sub(\" \", text)\n",
        "    mentions = re.findall(r\"@\\w+\", text)\n",
        "    text = re.sub(r\"@\\w+\", \" \", text)\n",
        "    text = re.sub(r\"(?!#)[^\\w\\s']\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text.lower(), {\"has_url\": bool(urls), \"mentions\": len(mentions)}\n",
        "\n",
        "# -------------------------\n",
        "# Hazard + Literal/figurative check\n",
        "# -------------------------\n",
        "def is_ocean_hazard_with_ml(text, sem_threshold=0.35):\n",
        "    text_clean, meta = clean_text_better(text)\n",
        "    if NEGATION_PATTERNS.search(text_clean):\n",
        "        return False, {\"reason\": \"negation_detected\", \"meta\": meta}\n",
        "\n",
        "    keyword_match = any(re.search(r\"\\b\" + re.escape(kw) + r\"\\b\", text_clean) for kw in ocean_hazard_keywords)\n",
        "    text_emb_np = embed_model.encode([text_clean], convert_to_numpy=True)\n",
        "    sem_sim = util.cos_sim(text_emb_np, hazard_embeddings).max().item()\n",
        "    sem_match = sem_sim >= sem_threshold\n",
        "\n",
        "    if not (keyword_match or sem_match):\n",
        "        return False, {\"reason\": \"no_keyword_or_semantic_match\", \"semantic_sim\": round(sem_sim, 2), \"meta\": meta}\n",
        "\n",
        "    if literal_clf is None:\n",
        "        return True, {\"reason\": \"Literal check skipped\", \"meta\": meta}\n",
        "\n",
        "    pca_object = literal_clf['pca']\n",
        "    model_object = literal_clf['model']\n",
        "    text_emb_transformed = pca_object.transform(text_emb_np)\n",
        "    literal_prob = model_object.predict_proba(text_emb_transformed)[0][1]\n",
        "\n",
        "    if literal_prob < 0.5:\n",
        "        return False, {\"reason\": \"figurative_detected\", \"literal_prob\": round(literal_prob, 2), \"meta\": meta}\n",
        "\n",
        "    success_meta = {\n",
        "        \"reason\": \"literal_hazard\",\n",
        "        \"keyword_match\": keyword_match,\n",
        "        \"semantic_sim\": round(sem_sim, 2),\n",
        "        \"literal_prob\": round(literal_prob, 2),\n",
        "        \"meta\": meta\n",
        "    }\n",
        "    return True, success_meta\n",
        "\n",
        "# -------------------------\n",
        "# Severity classifier\n",
        "# -------------------------\n",
        "def classify_severity_pipeline(texts, zero_shot_threshold=0.55, sim_threshold=0.4, explain=False):\n",
        "    candidate_labels = [\"Low severity ocean hazard\",\"Medium severity ocean hazard\",\"High severity ocean hazard\"]\n",
        "    results = []\n",
        "    for text in texts:\n",
        "        is_hazard, reason_meta = is_ocean_hazard_with_ml(text, sem_threshold=sim_threshold)\n",
        "        if not is_hazard:\n",
        "            if explain:\n",
        "                results.append((text, \"Not an ocean hazard\", 0.0, reason_meta))\n",
        "            else:\n",
        "                results.append((text, \"Not an ocean hazard\", 0.0))\n",
        "            continue\n",
        "\n",
        "        zs = classifier(text, candidate_labels)\n",
        "        best_label = zs[\"labels\"][0]\n",
        "        best_score = zs[\"scores\"][0]\n",
        "\n",
        "        severity_label, severity_score, method = \"Unclassified\", 0.0, None\n",
        "        if best_score >= zero_shot_threshold:\n",
        "            severity_label = best_label\n",
        "            severity_score = round(best_score * 100,2)\n",
        "            method = \"Zero-Shot\"\n",
        "        else:\n",
        "            text_emb = embed_model.encode(text, convert_to_tensor=True)\n",
        "            best_sim, best_sim_label = -1, None\n",
        "            for sev, emb in template_embeddings.items():\n",
        "                sim = util.cos_sim(text_emb, emb).max().item()\n",
        "                if sim > best_sim:\n",
        "                    best_sim = sim\n",
        "                    best_sim_label = sev\n",
        "            if best_sim >= sim_threshold:\n",
        "                severity_label = best_sim_label\n",
        "                severity_score = round(best_sim*100,2)\n",
        "                method = \"Semantic Similarity\"\n",
        "            else:\n",
        "                lang = detect(text)\n",
        "                if lang == \"en\":\n",
        "                    polarity = TextBlob(text).sentiment.polarity\n",
        "                    severity_label = \"Unclassified\"\n",
        "                    severity_score = round(abs(polarity)*10,2)\n",
        "                    method = \"Sentiment Fallback (English)\"\n",
        "                else:\n",
        "                    severity_label = \"Unclassified\"\n",
        "                    severity_score = 5.0\n",
        "                    method = f\"Sentiment Fallback (lang={lang})\"\n",
        "\n",
        "        if explain:\n",
        "            results.append((text, severity_label, severity_score, method, reason_meta))\n",
        "        else:\n",
        "            results.append((text, severity_label, severity_score))\n",
        "    return results\n",
        "\n",
        "# -------------------------\n",
        "# Location extraction\n",
        "# -------------------------\n",
        "def extract_location(text):\n",
        "    doc = nlp(text)\n",
        "    locations = [ent.text for ent in doc.ents if ent.label_ in [\"GPE\",\"LOC\"]]\n",
        "    return \", \".join(locations) if locations else None\n",
        "\n",
        "# -------------------------\n",
        "# Fetch Data from Supabase\n",
        "# -------------------------\n",
        "def fetch_twitter_data():\n",
        "    response = supabase.table(TABLE_NAME).select(\"*\").execute()\n",
        "    data = response.data\n",
        "    df = pd.DataFrame(data)\n",
        "    for col in [\"replies\",\"retweets\",\"likes\"]:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
        "    if \"created_at\" in df.columns:\n",
        "        df[\"created_at\"] = pd.to_datetime(df[\"created_at\"], errors='coerce')\n",
        "    return df\n",
        "\n",
        "# -------------------------\n",
        "# Graphs (same as previous)\n",
        "# -------------------------\n",
        "import io\n",
        "\n",
        "def save_and_upload(filename, supabase):\n",
        "    import io\n",
        "\n",
        "    buffer = io.BytesIO()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(buffer, format='png', dpi=150)\n",
        "    buffer.seek(0)\n",
        "\n",
        "    # ✅ Convert BytesIO to raw bytes\n",
        "    raw_bytes = buffer.read()\n",
        "\n",
        "    bucket_name = \"ReportImages\"\n",
        "    path = f\"analyticsGraphs/{filename}\"\n",
        "\n",
        "    # ✅ Use .update() with raw bytes instead of BytesIO\n",
        "    response = supabase.storage.from_(bucket_name).update(\n",
        "        path=path,\n",
        "        file=raw_bytes,\n",
        "        file_options={\"content-type\": \"image/png\"}\n",
        "    )\n",
        "\n",
        "    if hasattr(response, 'error') and response.error:\n",
        "        print(f\"❌ Failed to upload {filename}:\", response.error.message)\n",
        "    else:\n",
        "        print(f\"✅ Uploaded: {path}\")\n",
        "\n",
        "    buffer.close()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "def plot_graphs(df, severity_scores, supabase):\n",
        "    # Define your color scheme\n",
        "    colors = {\n",
        "        \"likes_vs_retweets\": \"#313D5A\",\n",
        "        \"replies_distribution\": \"#73628A\",\n",
        "        \"top10_tweets_by_engagement\": \"#CBC5EA\",\n",
        "        \"tweets_over_time\": \"#313D5A\",\n",
        "        \"severity_distribution\": \"#73628A\"\n",
        "    }\n",
        "\n",
        "    # 1) Likes vs Retweets\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(data=df, x=\"retweets\", y=\"likes\", alpha=0.6, s=60, color=colors[\"likes_vs_retweets\"])\n",
        "    plt.title(\"Likes vs Retweets\", fontsize=14)\n",
        "    plt.xlabel(\"Retweets\")\n",
        "    plt.ylabel(\"Likes\")\n",
        "    plt.grid(True)\n",
        "    save_and_upload(\"likes_vs_retweets.png\", supabase)\n",
        "\n",
        "    # 2) Replies Distribution\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.histplot(df[\"replies\"], bins=30, kde=True, color=colors[\"replies_distribution\"])\n",
        "    plt.xlabel(\"Replies\")\n",
        "    plt.ylabel(\"Number of Tweets\")\n",
        "    plt.title(\"Replies Distribution\", fontsize=14)\n",
        "    save_and_upload(\"replies_distribution.png\", supabase)\n",
        "\n",
        "    # 3) Top Tweets by Engagement\n",
        "    df[\"engagement\"] = df[\"likes\"] + df[\"retweets\"] + df[\"replies\"]\n",
        "    top = df.sort_values(\"engagement\", ascending=False).head(10)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=top[\"engagement\"], y=top[\"text\"].str.slice(0, 60), color=colors[\"top10_tweets_by_engagement\"])\n",
        "    plt.xlabel(\"Engagement\")\n",
        "    plt.title(\"Top 10 Tweets by Engagement\", fontsize=14)\n",
        "    plt.gca().invert_yaxis()\n",
        "    save_and_upload(\"top10_tweets_by_engagement.png\", supabase)\n",
        "\n",
        "    # 4) Tweets Over Time\n",
        "    if \"created_at\" in df.columns and not df[\"created_at\"].isnull().all():\n",
        "        ts = df.copy()\n",
        "        ts[\"created_at\"] = pd.to_datetime(ts[\"created_at\"])\n",
        "        ts.set_index(\"created_at\", inplace=True)\n",
        "        ts.sort_index(inplace=True)\n",
        "        ts_daily = ts[\"text\"].resample(\"D\").count().asfreq(\"D\", fill_value=0)\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(ts_daily.index, ts_daily.values, marker='o', linestyle='-', color=colors[\"tweets_over_time\"])\n",
        "        plt.title(\"Tweets Over Time\", fontsize=14)\n",
        "        plt.xlabel(\"Date\")\n",
        "        plt.ylabel(\"Number of Tweets\")\n",
        "        plt.xticks(rotation=30)\n",
        "        plt.grid(True)\n",
        "        save_and_upload(\"tweets_over_time.png\", supabase)\n",
        "    else:\n",
        "        print(\"No timestamp data available to plot tweets over time.\")\n",
        "\n",
        "    # 5) Severity Distribution\n",
        "    if severity_scores:\n",
        "        plt.figure(figsize=(7, 5))\n",
        "        sns.histplot(severity_scores, bins=[0, 25, 50, 75, 100], color=colors[\"severity_distribution\"], kde=True)\n",
        "        plt.xlabel(\"Severity Score\")\n",
        "        plt.ylabel(\"Number of Tweets\")\n",
        "        plt.title(\"Severity Distribution\", fontsize=14)\n",
        "        save_and_upload(\"severity_distribution.png\", supabase)\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    df = fetch_twitter_data()\n",
        "    print(\"Fetched\", len(df), \"rows from Supabase\")\n",
        "\n",
        "    # Initialize new columns\n",
        "    df[\"severity\"] = \"Not an ocean hazard\"\n",
        "    df[\"severity_score\"] = 0.0\n",
        "    df[\"isverified\"] = 0\n",
        "    df[\"location\"] = None\n",
        "\n",
        "    texts = df[\"text\"].fillna(\"\").tolist()\n",
        "    results = classify_severity_pipeline(texts, explain=True)\n",
        "\n",
        "    # Fill severity and isverified (robust unpacking)\n",
        "    for idx, r in enumerate(results):\n",
        "        if isinstance(r, (list, tuple)):\n",
        "            if len(r) == 5:\n",
        "                text, severity_label, severity_score, method, reason_meta = r\n",
        "            elif len(r) == 4:\n",
        "                text, severity_label, severity_score, reason_meta = r\n",
        "                method = None\n",
        "            elif len(r) == 3:\n",
        "                text, severity_label, severity_score = r\n",
        "                method = None\n",
        "                reason_meta = None\n",
        "            else:\n",
        "                text = str(r[0]) if len(r) > 0 else \"\"\n",
        "                severity_label = \"Unclassified\"\n",
        "                severity_score = 0.0\n",
        "                method = None\n",
        "                reason_meta = None\n",
        "        else:\n",
        "            text = \"\"\n",
        "            severity_label = \"Unclassified\"\n",
        "            severity_score = 0.0\n",
        "            method = None\n",
        "            reason_meta = None\n",
        "\n",
        "        df.at[idx, \"severity\"] = severity_label\n",
        "        df.at[idx, \"severity_score\"] = float(severity_score)\n",
        "        if severity_label != \"Not an ocean hazard\":\n",
        "            df.at[idx, \"isverified\"] = 1\n",
        "        df.at[idx, \"location\"] = extract_location(text)\n",
        "\n",
        "    # Update Supabase table (twitterdata)\n",
        "    for idx, row in df.iterrows():\n",
        "        supabase.table(TABLE_NAME).update({\n",
        "            \"severity\": row[\"severity\"],\n",
        "            \"severity_score\": float(row[\"severity_score\"]),\n",
        "            \"isverified\": int(row[\"isverified\"]),\n",
        "            \"location\": row[\"location\"]\n",
        "        }).eq(\"text\", row[\"text\"]).execute()\n",
        "\n",
        "    # -------------------------\n",
        "    # Process reports table\n",
        "    # -------------------------\n",
        "    try:\n",
        "        resp = supabase.table(\"reports\").select(\"*\").execute()\n",
        "        reports_data = resp.data\n",
        "        df_reports = pd.DataFrame(reports_data)\n",
        "\n",
        "        if df_reports.empty:\n",
        "            print(\"No rows in reports table to process.\")\n",
        "        else:\n",
        "            if \"description\" not in df_reports.columns:\n",
        "                print(\"reports table does not contain 'description' column. Skipping reports processing.\")\n",
        "            else:\n",
        "                df_reports[\"severity\"] = \"Not an ocean hazard\"\n",
        "                df_reports[\"severity_score\"] = 0.0\n",
        "\n",
        "                descriptions = df_reports[\"description\"].fillna(\"\").tolist()\n",
        "                rep_results = classify_severity_pipeline(descriptions, explain=True)\n",
        "\n",
        "                for idx, r in enumerate(rep_results):\n",
        "                    if isinstance(r, (list, tuple)):\n",
        "                        if len(r) == 5:\n",
        "                            text, severity_label, severity_score, method, reason_meta = r\n",
        "                        elif len(r) == 4:\n",
        "                            text, severity_label, severity_score, reason_meta = r\n",
        "                            method = None\n",
        "                        elif len(r) == 3:\n",
        "                            text, severity_label, severity_score = r\n",
        "                            method = None\n",
        "                            reason_meta = None\n",
        "                        else:\n",
        "                            text = str(r[0]) if len(r) > 0 else \"\"\n",
        "                            severity_label = \"Unclassified\"\n",
        "                            severity_score = 0.0\n",
        "                            method = None\n",
        "                            reason_meta = None\n",
        "                    else:\n",
        "                        text = \"\"\n",
        "                        severity_label = \"Unclassified\"\n",
        "                        severity_score = 0.0\n",
        "                        method = None\n",
        "                        reason_meta = None\n",
        "\n",
        "                    df_reports.at[idx, \"severity\"] = severity_label\n",
        "                    df_reports.at[idx, \"severity_score\"] = float(severity_score)\n",
        "\n",
        "                use_id = \"id\" in df_reports.columns\n",
        "                for idx, row in df_reports.iterrows():\n",
        "                    update_payload = {\n",
        "                        \"severity\": row[\"severity\"],\n",
        "                        \"severity_score\": float(row[\"severity_score\"])\n",
        "                    }\n",
        "                    if use_id:\n",
        "                        supabase.table(\"reports\").update(update_payload).eq(\"id\", int(row[\"id\"])).execute()\n",
        "                    else:\n",
        "                        supabase.table(\"reports\").update(update_payload).eq(\"description\", row[\"description\"]).execute()\n",
        "\n",
        "                print(f\"Processed and updated {len(df_reports)} rows in 'reports' table.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error processing reports table:\", e)\n",
        "\n",
        "    # -------------------------\n",
        "    # Upload graphs to Supabase storage\n",
        "    # -------------------------\n",
        "    severity_scores = df[df[\"isverified\"] == 1][\"severity_score\"].tolist()\n",
        "    plot_graphs(df, severity_scores,supabase)\n",
        "\n",
        "    print(\"✅ All graphs uploaded to Supabase storage: ReportImages/analyticsGraphs/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXpIAvajwikh",
        "outputId": "e61f2d42-2a92-4dbc-9880-05cd87e4de60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetched 11 rows from Supabase\n",
            "Processed and updated 8 rows in 'reports' table.\n",
            "✅ Uploaded: analyticsGraphs/likes_vs_retweets.png\n",
            "✅ Uploaded: analyticsGraphs/replies_distribution.png\n",
            "✅ Uploaded: analyticsGraphs/top10_tweets_by_engagement.png\n",
            "✅ Uploaded: analyticsGraphs/tweets_over_time.png\n",
            "✅ Uploaded: analyticsGraphs/severity_distribution.png\n",
            "✅ All graphs uploaded to Supabase storage: ReportImages/analyticsGraphs/\n"
          ]
        }
      ]
    }
  ]
}